# **MoST â€” Mixture of Socratic Thoughts**

*A tri-agent iterative reasoning framework for improving LLM reliability.*

MoST is an experimental reasoning architecture designed to improve an LLMâ€™s accuracy, stability, and step-by-step reasoning through a structured multi-agent loop.
It is built by a young developer (age 13), so the project evolves rapidly as experiments continue.

---

## â­ **What is MoST?**

MoST stands for **Mixture of Socratic Thoughts** â€” a system where multiple â€œthinking agentsâ€ collaborate to solve a task more reliably than a single LLM.

The core idea:

> Instead of asking an LLM to answer directly, split the thinking process into *roles* and let them collaborate under supervision.

The MVP (Minimum Viable Product) of MoST uses **three agents**:

### âœ… **1. U-LM (User-Like Model)**

Rephrases each step as a natural language instruction, acting like a questioning partner.

### âœ… **2. R-LM (Responder Model)**

Attempts to answer the U-LMâ€™s question.

### âœ… **3. Meta-Agent** *(Coming soon)*

Supervises the turn, checks for correctness, and decides whether:

* the step is complete
* R-LM hallucinated or drifted
* the U-LM needs to ask a better question
* the loop should retry or move on

This creates a **controlled feedback loop**:

```
U-LM â†’ R-LM â†’ Meta-Agent â†’ U-LM â†’ R-LM â†’ ...
```

â€¦until the Meta-Agent decides the step is finished.

---

## ğŸ§  **Why MoST?**

Large Language Models often struggle with:

* multi-step reasoning
* task consistency
* state tracking
* hallucinations
* long chain-of-thought drift

MoST attempts to fix this by *splitting* the reasoning into roles and supervising each turn.

Even small improvements over the baseline LLM count â€” this is research and experimentation, not a finished product.

---

## ğŸ“ **Project Structure**

```
MoST/
â”œâ”€â”€ main.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ lm.py
â”‚   â”œâ”€â”€ loading.py
â”‚   â”œâ”€â”€ clean_output.py
â”‚   â”œâ”€â”€ colourized_logs.py
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ planner/
â”‚   â”‚   â””â”€â”€ planner.md
â”‚   â”œâ”€â”€ u_lm/
â”‚   â”‚   â””â”€â”€ u_lm.md
â”‚   â”œâ”€â”€ r_lm/
â”‚   â”‚   â””â”€â”€ r_lm.md
â”‚   â””â”€â”€ meta_agent/
â”‚       â””â”€â”€ meta_agent.md   (coming soon)
â””â”€â”€ README.md
```

---

## ğŸš€ **Running the Project**

Make sure you're inside the virtual environment, then run:

```bash
python main.py
```

The process:

1. The **Planner** produces a step-by-step reasoning plan.
2. For each step:

   * U-LM reformulates the step as a question.
   * R-LM answers it.
   * *(Soon)* Meta-Agent judges correctness.
3. The system continues until all steps are complete.

Logs are colourized and show each agentâ€™s output.

---

## ğŸ§ª Example Tasks

MoST has been tested on:

* word breakdown tasks
* counting tasks
* classic logic riddles (e.g., wolfâ€“goatâ€“cabbage)
* simple reasoning problems

Early tests show promising improvements when Meta-Agent supervision is added.

---

## ğŸ”® **Roadmap**

MoST MVP (now)

âœ… Planner
âœ… U-LM
âœ… R-LM
ğŸŸ¥ Meta-Agent (in progress)
ğŸŸ¥ Step completion detection
ğŸŸ¥ Retry + correction loop

Planned additions:

* âœ… **Critics (Factual, Logical, Ethical, Creative)**
* âœ… **Ensemble scoring / confidence aggregation**
* âœ… **CGS â€“ Context-Grabbing System**
* âœ… **LAMS â€“ Long-Term Memory System**
* âœ… **Background Mind / reflective processes**
* âœ… **Improved state tracking and grounding**

MoST will grow in complexity over time.

---

## ğŸ¤ Contributing

This is an experimental research project.
Pull requests, suggestions, and improvements are welcome â€” just be kind and constructive.

---

## âš ï¸ Disclaimer

MoST is an independent research project created for fun and learning.
It is *not* a commercial product, and it may break, hallucinate, or behave unpredictably.
The author is 13, so some design choices are intentionally simple or experimental.